
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json
import time
import re
import os
import logging

# Configurar logging para o scraper
logger = logging.getLogger(__name__)

def setup_driver():
    logger.info("ğŸ”§ Configurando opÃ§Ãµes do Chrome...")
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-plugins")
    chrome_options.add_argument("--disable-images")
    chrome_options.add_argument("--disable-javascript")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    # Usar webdriver-manager para gerenciar o ChromeDriver automaticamente
    logger.info("ğŸ“¥ Instalando/configurando ChromeDriver...")
    try:
        service = Service(ChromeDriverManager().install())
        logger.info("âœ… ChromeDriver configurado via webdriver-manager")
    except Exception as e:
        logger.warning(f"âš ï¸ Falha no webdriver-manager: {str(e)}")
        logger.info("ğŸ”„ Tentando caminho local do ChromeDriver...")
        # Fallback para caminho local se webdriver-manager falhar
        service = Service("/opt/homebrew/bin/chromedriver")
        logger.info("âœ… Usando ChromeDriver local")
    
    try:
        driver = webdriver.Chrome(service=service, options=chrome_options)
        logger.info("ğŸš€ Driver do Chrome iniciado com sucesso")
        return driver
    except ImportError as e:
        logger.error(f"Erro de importaÃ§Ã£o ao configurar driver: {e}")
        logger.error("Verifique se o Chrome estÃ¡ instalado e o webdriver-manager estÃ¡ disponÃ­vel")
        return None
    except FileNotFoundError as e:
        logger.error(f"ChromeDriver nÃ£o encontrado: {e}")
        logger.error("Tentando usar ChromeDriver do sistema...")
        return None
    except Exception as e:
        logger.error(f"Erro inesperado ao configurar driver: {e}")
        logger.error(f"Tipo do erro: {type(e).__name__}")
        return None

def clean_price(price_str):
    if not price_str: return 'N/A'
    # Remove thousand separators ("." or ",") and replace decimal comma (",") with dot (".")
    # Handles cases like '1.165,00' -> '1165.00' and '97,00' -> '97.00'
    # First, remove all dots (thousand separator in some locales)
    cleaned_price = price_str.replace('.', '')
    # Then, replace comma with dot (decimal separator in some locales)
    cleaned_price = cleaned_price.replace(',', '.')
    
    # Check if the cleaned price matches a pattern like '12345' (integer without decimal)
    # If so, and it's a dollar price, assume it should have two decimal places
    # This is a heuristic based on the observation that some dollar prices are missing decimal
    if re.fullmatch(r'\d+', cleaned_price) and len(cleaned_price) > 2: # e.g., '9700' should be '97.00'
        cleaned_price = cleaned_price[:-2] + '.' + cleaned_price[-2:]

    # Ensure it's a valid number format
    try:
        return str(float(cleaned_price))
    except ValueError:
        return 'N/A'

def scrape_products(driver, search_term):
    logger.info(f"ğŸŒ Iniciando scraping para termo: '{search_term}'")
    base_url = "https://comprasparaguai.com.br"
    search_url = f"{base_url}/busca/?q={search_term}"
    
    logger.info(f"ğŸ“ Navegando para: {search_url}")
    try:
        driver.get(search_url)
        logger.info("âœ… PÃ¡gina carregada com sucesso")
    except TimeoutException as e:
        logger.error(f"â° Timeout ao carregar pÃ¡gina: {str(e)}")
        logger.error("A pÃ¡gina demorou muito para carregar. Verifique a conexÃ£o com a internet.")
        raise
    except WebDriverException as e:
        logger.error(f"ğŸš« Erro do WebDriver ao carregar pÃ¡gina: {str(e)}")
        logger.error("PossÃ­vel problema com o navegador ou driver.")
        raise
    except Exception as e:
        logger.error(f"âŒ Erro inesperado ao carregar pÃ¡gina: {str(e)}")
        logger.error(f"Tipo do erro: {type(e).__name__}")
        raise

    # Aceitar cookies, se presente
    logger.info("ğŸª Verificando banner de cookies...")
    try:
        cookie_button = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ENTENDI')] | //button[contains(text(), 'Estou de Acordo')] | //*[@id='btn-cookie-allow']"))
        )
        cookie_button.click()
        logger.info("âœ… Banner de cookies aceito")
        time.sleep(2) # Dar um tempo para o banner de cookies sumir
    except Exception as e:
        logger.info("â„¹ï¸ Banner de cookies nÃ£o encontrado ou jÃ¡ aceito")

    products_data = []
    
    # Rolar a pÃ¡gina para carregar mais produtos (se houver lazy loading)
    logger.info("ğŸ“œ Iniciando scroll para carregar produtos...")
    last_height = driver.execute_script("return document.body.scrollHeight")
    scroll_attempts = 0
    max_scroll_attempts = 5 # Ajustar conforme necessÃ¡rio
    
    while scroll_attempts < max_scroll_attempts:
        logger.debug(f"ğŸ“œ Scroll tentativa {scroll_attempts + 1}/{max_scroll_attempts}")
        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        time.sleep(3)  # Esperar o conteÃºdo carregar
        new_height = driver.execute_script("return document.body.scrollHeight")
        if new_height == last_height:
            logger.info("ğŸ“œ Fim da pÃ¡gina atingido")
            break
        last_height = new_height
        scroll_attempts += 1
    
    logger.info(f"ğŸ“œ Scroll concluÃ­do apÃ³s {scroll_attempts} tentativas")

    # Salvar o HTML da pÃ¡gina para depuraÃ§Ã£o
    logger.info("ğŸ’¾ Salvando HTML da pÃ¡gina para debug...")
    try:
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        logger.info("âœ… HTML salvo em debug_page.html")
    except Exception as e:
        logger.warning(f"âš ï¸ Erro ao salvar HTML de debug: {str(e)}")

    # Usar BeautifulSoup para parsear o HTML apÃ³s o carregamento dinÃ¢mico
    logger.info("ğŸ² Parseando HTML com BeautifulSoup...")
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        logger.info("âœ… HTML parseado com sucesso")
    except Exception as e:
        logger.error(f"âŒ Erro ao parsear HTML com BeautifulSoup: {str(e)}")
        logger.error("PossÃ­vel problema com o conteÃºdo HTML ou parser")
        # Tentar com parser alternativo
        try:
            logger.info("ğŸ”„ Tentando parser alternativo...")
            soup = BeautifulSoup(driver.page_source, 'lxml')
            logger.info("âœ… HTML parseado com parser lxml")
        except Exception as e2:
            logger.error(f"âŒ Erro tambÃ©m com parser lxml: {str(e2)}")
            try:
                logger.info("ğŸ”„ Tentando parser html.parser bÃ¡sico...")
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                logger.info("âœ… HTML parseado com parser bÃ¡sico")
            except Exception as e3:
                logger.error(f"âŒ Falha total no parsing: {str(e3)}")
                raise Exception("NÃ£o foi possÃ­vel parsear o HTML com nenhum parser disponÃ­vel")

    # Extrair dados dos produtos
    logger.info("ğŸ” Procurando produtos na pÃ¡gina...")
    product_items = soup.find_all('div', class_='promocao-produtos-item')
    logger.info(f"ğŸ“Š Encontrados {len(product_items)} itens de produto")

    if len(product_items) == 0:
        logger.warning("âš ï¸ Nenhum produto encontrado! Verificando estrutura da pÃ¡gina...")
        # Tentar encontrar outros seletores possÃ­veis
        alternative_selectors = [
            'div.produto-item',
            'div.product-item', 
            'div[class*="produto"]',
            'div[class*="product"]',
            'article',
            '.card'
        ]
        
        for selector in alternative_selectors:
            try:
                items = soup.select(selector)
                if items:
                    logger.info(f"ğŸ” Encontrados {len(items)} itens com seletor alternativo: {selector}")
                    break
            except Exception as e:
                logger.debug(f"Seletor {selector} falhou: {str(e)}")

    for i, item in enumerate(product_items):
        logger.debug(f"ğŸ” Processando produto {i+1}/{len(product_items)}")
        try:
            name_tag = item.find('div', class_='promocao-item-nome')
            name = name_tag.a.text.strip() if name_tag and name_tag.a else 'N/A'
            logger.debug(f"ğŸ“ Nome: {name}")

            link_tag = item.find('div', class_='promocao-item-nome')
            link = link_tag.a['href'] if link_tag and link_tag.a and 'href' in link_tag.a.attrs else 'N/A'
            if link and not link.startswith('http'):
                link = base_url + link
            logger.debug(f"ğŸ”— Link: {link}")

            price_usd = 'N/A'
            price_brl = 'N/A'

            # ExtraÃ§Ã£o do preÃ§o em dÃ³lar
            price_usd_element = item.select_one('.price-model span') or item.select_one('.promocao-item-preco-oferta strong')
            if price_usd_element:
                price_usd_text = price_usd_element.get_text(strip=True)
                match_usd = re.search(r'US\$\s*([\d\.,]+)', price_usd_text)
                if match_usd:
                    price_usd = clean_price(match_usd.group(1))
                    logger.debug(f"ğŸ’µ PreÃ§o USD: {price_usd}")

            # ExtraÃ§Ã£o do preÃ§o em real
            price_brl_element = item.select_one('.promocao-item-preco-text')
            if price_brl_element:
                price_brl_text = price_brl_element.get_text(strip=True)
                match_brl = re.search(r'R\$\s*([\d\.,]+)', price_brl_text)
                if match_brl:
                    price_brl = clean_price(match_brl.group(1))
                    logger.debug(f"ğŸ’° PreÃ§o BRL: {price_brl}")

            # ExtraÃ§Ã£o da URL da imagem
            image_url = 'N/A'
            img_tag = item.select_one('.promocao-item-img img')
            if img_tag:
                if 'data-src' in img_tag.attrs:
                    image_url = img_tag['data-src']
                elif 'src' in img_tag.attrs:
                    image_url = img_tag['src']
                
                # Corrigir URLs relativas ou incompletas
                if image_url and not image_url.startswith('http') and not image_url.startswith('//'):
                    image_url = 'https://media-production-bucket.us-southeast-1.linodeobjects.com' + image_url
                elif image_url.startswith('//'):
                    image_url = 'https:' + image_url
                
                # Se a URL da imagem ainda for 'N/A' ou vazia, garantir que nÃ£o seja concatenada com o base_url
                if not image_url or 'N/A' in image_url:
                    image_url = 'N/A'
                    
                logger.debug(f"ğŸ–¼ï¸ Imagem: {image_url}")

            product_data = {
                'Nome': name,
                'PreÃ§o (US$)': price_usd,
                'PreÃ§o (R$)': price_brl,
                'Link': link,
                'Imagem': image_url
            }
            
            products_data.append(product_data)
            logger.debug(f"âœ… Produto {i+1} processado com sucesso")
            
        except AttributeError as e:
            logger.error(f"âŒ Erro de atributo ao extrair produto {i+1}: {str(e)}")
            logger.error("PossÃ­vel mudanÃ§a na estrutura HTML da pÃ¡gina")
            continue
        except KeyError as e:
            logger.error(f"âŒ Chave nÃ£o encontrada ao extrair produto {i+1}: {str(e)}")
            logger.error("Elemento HTML esperado nÃ£o possui o atributo necessÃ¡rio")
            continue
        except TypeError as e:
            logger.error(f"âŒ Erro de tipo ao extrair produto {i+1}: {str(e)}")
            logger.error("Tipo de dados inesperado durante a extraÃ§Ã£o")
            continue
        except Exception as e:
            logger.error(f"âŒ Erro inesperado ao extrair produto {i+1}: {str(e)}")
            logger.error(f"Tipo do erro: {type(e).__name__}")
            continue
    
    logger.info(f"ğŸ¯ Scraping finalizado. Total de {len(products_data)} produtos extraÃ­dos")
    return products_data

def save_to_excel(data, filename='produtos.xlsx'):
    try:
        import pandas as pd
        df = pd.DataFrame(data)
        df.to_excel(filename, index=False)
        print(f"Dados salvos em {filename}")
    except ImportError:
        print("Pandas nÃ£o disponÃ­vel. Salvando apenas em JSON.")
        save_to_json(data, filename.replace('.xlsx', '.json'))

def save_to_json(data, filename='produtos.json'):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Dados salvos em {filename}")

if __name__ == '__main__':
    search_term = 'iphone' # Termo de busca padrÃ£o
    print(f"Iniciando a extraÃ§Ã£o para: {search_term}")
    driver = setup_driver()
    try:
        products = scrape_products(driver, search_term)
        if products:
            save_to_excel(products)
            save_to_json(products)
            print(f"Total de {len(products)} produtos extraÃ­dos.")
        else:
            print("Nenhum produto encontrado.")
    finally:
        driver.quit()

