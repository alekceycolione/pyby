
import logging
import requests
from bs4 import BeautifulSoup
import json
import time
import re
import os
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, NoSuchElementException

try:
    from webdriver_manager.chrome import ChromeDriverManager
except ImportError as e:
    logger = logging.getLogger(__name__)
    logger.warning(f"‚ö†Ô∏è webdriver_manager n√£o dispon√≠vel: {str(e)}")
    ChromeDriverManager = None

# Configurar logging para o scraper
logger = logging.getLogger(__name__)

def setup_driver():
    logger.info("üîß Configurando op√ß√µes do Chrome...")
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument("--disable-plugins")
    chrome_options.add_argument("--disable-images")
    chrome_options.add_argument("--disable-javascript")
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36")
    
    # Tentar m√∫ltiplas estrat√©gias para configurar o driver
    driver = None
    
    # Estrat√©gia 1: webdriver-manager (se dispon√≠vel)
    if ChromeDriverManager is not None:
        logger.info("üì• Tentativa 1: Instalando/configurando ChromeDriver via webdriver-manager...")
        try:
            service = Service(ChromeDriverManager().install())
            driver = webdriver.Chrome(service=service, options=chrome_options)
            logger.info("‚úÖ ChromeDriver configurado via webdriver-manager")
            return driver
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Falha na tentativa 1: {str(e)}")
    else:
        logger.info("‚è≠Ô∏è Pulando tentativa 1: webdriver-manager n√£o dispon√≠vel")
    
    # Estrat√©gia 2: Caminho padr√£o do Render
    logger.info("üîÑ Tentativa 2: Usando caminho padr√£o do Render...")
    try:
        service = Service("/usr/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        logger.info("‚úÖ ChromeDriver configurado no caminho do Render")
        return driver
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Falha na tentativa 2: {str(e)}")
    
    # Estrat√©gia 3: Caminho local (para desenvolvimento)
    logger.info("üîÑ Tentativa 3: Usando caminho local...")
    try:
        service = Service("/opt/homebrew/bin/chromedriver")
        driver = webdriver.Chrome(service=service, options=chrome_options)
        logger.info("‚úÖ ChromeDriver configurado no caminho local")
        return driver
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Falha na tentativa 3: {str(e)}")
    
    # Estrat√©gia 4: Sem service espec√≠fico
    logger.info("üîÑ Tentativa 4: Usando driver sem service espec√≠fico...")
    try:
        driver = webdriver.Chrome(options=chrome_options)
        logger.info("‚úÖ ChromeDriver configurado sem service espec√≠fico")
        return driver
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Falha na tentativa 4: {str(e)}")
    
    # Se todas as tentativas falharam, implementar fallback sem Selenium
    logger.error("‚ùå Todas as tentativas de configurar o ChromeDriver falharam")
    logger.info("üîÑ Implementando fallback: scraping sem Selenium usando requests")
    
    # Retornar um objeto mock que simula o driver para o fallback
    class MockDriver:
        def __init__(self):
            self.session = requests.Session()
            self.session.headers.update({
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
            })
            logger.info("‚úÖ Fallback configurado: usando requests em vez de Selenium")
        
        def get(self, url):
            logger.info(f"üåê Fallback: Fazendo requisi√ß√£o HTTP para {url}")
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            self.page_source = response.text
            logger.info("‚úÖ Fallback: P√°gina carregada com sucesso")
        
        def quit(self):
            logger.info("üîí Fallback: Fechando sess√£o requests")
            self.session.close()
        
        def find_element(self, by, value):
            # M√©todo dummy para compatibilidade
            raise NoSuchElementException(f"Fallback mode: elemento {value} n√£o encontrado")
        
        def find_elements(self, by, value):
            # M√©todo dummy para compatibilidade
            return []
    
    return MockDriver()

def clean_price(price_str):
    if not price_str: return 'N/A'
    # Remove thousand separators ("." or ",") and replace decimal comma (",") with dot (".")
    # Handles cases like '1.165,00' -> '1165.00' and '97,00' -> '97.00'
    # First, remove all dots (thousand separator in some locales)
    cleaned_price = price_str.replace('.', '')
    # Then, replace comma with dot (decimal separator in some locales)
    cleaned_price = cleaned_price.replace(',', '.')
    
    # Check if the cleaned price matches a pattern like '12345' (integer without decimal)
    # If so, and it's a dollar price, assume it should have two decimal places
    # This is a heuristic based on the observation that some dollar prices are missing decimal
    if re.fullmatch(r'\d+', cleaned_price) and len(cleaned_price) > 2: # e.g., '9700' should be '97.00'
        cleaned_price = cleaned_price[:-2] + '.' + cleaned_price[-2:]

    # Ensure it's a valid number format
    try:
        return str(float(cleaned_price))
    except ValueError:
        return 'N/A'

def scrape_products(driver, search_term):
    logger.info(f"üåê Iniciando scraping para termo: '{search_term}'")
    base_url = "https://comprasparaguai.com.br"
    search_url = f"{base_url}/busca/?q={search_term}"
    
    logger.info(f"üìç Navegando para: {search_url}")
    try:
        driver.get(search_url)
        logger.info("‚úÖ P√°gina carregada com sucesso")
    except TimeoutException as e:
        logger.error(f"‚è∞ Timeout ao carregar p√°gina: {str(e)}")
        logger.error("A p√°gina demorou muito para carregar. Verifique a conex√£o com a internet.")
        raise
    except WebDriverException as e:
        logger.error(f"üö´ Erro do WebDriver ao carregar p√°gina: {str(e)}")
        logger.error("Poss√≠vel problema com o navegador ou driver.")
        raise
    except Exception as e:
        logger.error(f"‚ùå Erro inesperado ao carregar p√°gina: {str(e)}")
        logger.error(f"Tipo do erro: {type(e).__name__}")
        raise

    # Aceitar cookies, se presente (apenas para Selenium real)
    logger.info("üç™ Verificando banner de cookies...")
    try:
        # Verificar se √© o MockDriver (fallback)
        if hasattr(driver, 'session'):
            logger.info("‚è≠Ô∏è Fallback mode: pulando verifica√ß√£o de cookies")
        else:
            cookie_button = WebDriverWait(driver, 10).until(
                EC.element_to_be_clickable((By.XPATH, "//button[contains(text(), 'ENTENDI')] | //button[contains(text(), 'Estou de Acordo')] | //*[@id='btn-cookie-allow']"))
            )
            cookie_button.click()
            logger.info("‚úÖ Banner de cookies aceito")
        time.sleep(2) # Dar um tempo para o banner de cookies sumir
    except Exception as e:
        logger.info("‚ÑπÔ∏è Banner de cookies n√£o encontrado ou j√° aceito")

    products_data = []
    
    # Rolar a p√°gina para carregar mais produtos (apenas para Selenium real)
    logger.info("üìú Iniciando scroll para carregar produtos...")
    if hasattr(driver, 'session'):
        logger.info("‚è≠Ô∏è Fallback mode: pulando scroll (usando HTML est√°tico)")
    else:
        last_height = driver.execute_script("return document.body.scrollHeight")
        scroll_attempts = 0
        max_scroll_attempts = 5 # Ajustar conforme necess√°rio
        
        while scroll_attempts < max_scroll_attempts:
            logger.debug(f"üìú Scroll tentativa {scroll_attempts + 1}/{max_scroll_attempts}")
            driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(3)  # Esperar o conte√∫do carregar
            new_height = driver.execute_script("return document.body.scrollHeight")
            if new_height == last_height:
                logger.info("üìú Fim da p√°gina atingido")
                break
            last_height = new_height
            scroll_attempts += 1
        
        logger.info(f"üìú Scroll conclu√≠do ap√≥s {scroll_attempts} tentativas")

    # Salvar o HTML da p√°gina para depura√ß√£o
    logger.info("üíæ Salvando HTML da p√°gina para debug...")
    try:
        with open("debug_page.html", "w", encoding="utf-8") as f:
            f.write(driver.page_source)
        logger.info("‚úÖ HTML salvo em debug_page.html")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao salvar HTML de debug: {str(e)}")

    # Usar BeautifulSoup para parsear o HTML ap√≥s o carregamento din√¢mico
    logger.info("üç≤ Parseando HTML com BeautifulSoup...")
    try:
        soup = BeautifulSoup(driver.page_source, 'html.parser')
        logger.info("‚úÖ HTML parseado com sucesso")
    except Exception as e:
        logger.error(f"‚ùå Erro ao parsear HTML com BeautifulSoup: {str(e)}")
        logger.error("Poss√≠vel problema com o conte√∫do HTML ou parser")
        # Tentar com parser alternativo
        try:
            logger.info("üîÑ Tentando parser alternativo...")
            soup = BeautifulSoup(driver.page_source, 'lxml')
            logger.info("‚úÖ HTML parseado com parser lxml")
        except Exception as e2:
            logger.error(f"‚ùå Erro tamb√©m com parser lxml: {str(e2)}")
            try:
                logger.info("üîÑ Tentando parser html.parser b√°sico...")
                soup = BeautifulSoup(driver.page_source, 'html.parser')
                logger.info("‚úÖ HTML parseado com parser b√°sico")
            except Exception as e3:
                logger.error(f"‚ùå Falha total no parsing: {str(e3)}")
                raise Exception("N√£o foi poss√≠vel parsear o HTML com nenhum parser dispon√≠vel")

    # Extrair dados dos produtos
    logger.info("üîç Procurando produtos na p√°gina...")
    product_items = soup.find_all('div', class_='promocao-produtos-item')
    logger.info(f"üìä Encontrados {len(product_items)} itens de produto")

    if len(product_items) == 0:
        logger.warning("‚ö†Ô∏è Nenhum produto encontrado! Verificando estrutura da p√°gina...")
        # Tentar encontrar outros seletores poss√≠veis
        alternative_selectors = [
            'div.produto-item',
            'div.product-item', 
            'div[class*="produto"]',
            'div[class*="product"]',
            'article',
            '.card'
        ]
        
        for selector in alternative_selectors:
            try:
                items = soup.select(selector)
                if items:
                    logger.info(f"üîç Encontrados {len(items)} itens com seletor alternativo: {selector}")
                    break
            except Exception as e:
                logger.debug(f"Seletor {selector} falhou: {str(e)}")

    for i, item in enumerate(product_items):
        logger.debug(f"üîç Processando produto {i+1}/{len(product_items)}")
        try:
            name_tag = item.find('div', class_='promocao-item-nome')
            name = name_tag.a.text.strip() if name_tag and name_tag.a else 'N/A'
            logger.debug(f"üìù Nome: {name}")

            link_tag = item.find('div', class_='promocao-item-nome')
            link = link_tag.a['href'] if link_tag and link_tag.a and 'href' in link_tag.a.attrs else 'N/A'
            if link and not link.startswith('http'):
                link = base_url + link
            logger.debug(f"üîó Link: {link}")

            price_usd = 'N/A'
            price_brl = 'N/A'

            # Extra√ß√£o do pre√ßo em d√≥lar
            price_usd_element = item.select_one('.price-model span') or item.select_one('.promocao-item-preco-oferta strong')
            if price_usd_element:
                price_usd_text = price_usd_element.get_text(strip=True)
                match_usd = re.search(r'US\$\s*([\d\.,]+)', price_usd_text)
                if match_usd:
                    price_usd = clean_price(match_usd.group(1))
                    logger.debug(f"üíµ Pre√ßo USD: {price_usd}")

            # Extra√ß√£o do pre√ßo em real
            price_brl_element = item.select_one('.promocao-item-preco-text')
            if price_brl_element:
                price_brl_text = price_brl_element.get_text(strip=True)
                match_brl = re.search(r'R\$\s*([\d\.,]+)', price_brl_text)
                if match_brl:
                    price_brl = clean_price(match_brl.group(1))
                    logger.debug(f"üí∞ Pre√ßo BRL: {price_brl}")

            # Extra√ß√£o da URL da imagem
            image_url = 'N/A'
            img_tag = item.select_one('.promocao-item-img img')
            if img_tag:
                if 'data-src' in img_tag.attrs:
                    image_url = img_tag['data-src']
                elif 'src' in img_tag.attrs:
                    image_url = img_tag['src']
                
                # Corrigir URLs relativas ou incompletas
                if image_url and not image_url.startswith('http') and not image_url.startswith('//'):
                    image_url = 'https://media-production-bucket.us-southeast-1.linodeobjects.com' + image_url
                elif image_url.startswith('//'):
                    image_url = 'https:' + image_url
                
                # Se a URL da imagem ainda for 'N/A' ou vazia, garantir que n√£o seja concatenada com o base_url
                if not image_url or 'N/A' in image_url:
                    image_url = 'N/A'
                    
                logger.debug(f"üñºÔ∏è Imagem: {image_url}")

            product_data = {
                'Nome': name,
                'Pre√ßo (US$)': price_usd,
                'Pre√ßo (R$)': price_brl,
                'Link': link,
                'Imagem': image_url
            }
            
            products_data.append(product_data)
            logger.debug(f"‚úÖ Produto {i+1} processado com sucesso")
            
        except AttributeError as e:
            logger.error(f"‚ùå Erro de atributo ao extrair produto {i+1}: {str(e)}")
            logger.error("Poss√≠vel mudan√ßa na estrutura HTML da p√°gina")
            continue
        except KeyError as e:
            logger.error(f"‚ùå Chave n√£o encontrada ao extrair produto {i+1}: {str(e)}")
            logger.error("Elemento HTML esperado n√£o possui o atributo necess√°rio")
            continue
        except TypeError as e:
            logger.error(f"‚ùå Erro de tipo ao extrair produto {i+1}: {str(e)}")
            logger.error("Tipo de dados inesperado durante a extra√ß√£o")
            continue
        except Exception as e:
            logger.error(f"‚ùå Erro inesperado ao extrair produto {i+1}: {str(e)}")
            logger.error(f"Tipo do erro: {type(e).__name__}")
            continue
    
    logger.info(f"üéØ Scraping finalizado. Total de {len(products_data)} produtos extra√≠dos")
    return products_data

def save_to_excel(data, filename='produtos.xlsx'):
    try:
        import pandas as pd
        df = pd.DataFrame(data)
        df.to_excel(filename, index=False)
        print(f"Dados salvos em {filename}")
    except ImportError:
        print("Pandas n√£o dispon√≠vel. Salvando apenas em JSON.")
        save_to_json(data, filename.replace('.xlsx', '.json'))

def save_to_json(data, filename='produtos.json'):
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"Dados salvos em {filename}")

if __name__ == '__main__':
    search_term = 'iphone' # Termo de busca padr√£o
    print(f"Iniciando a extra√ß√£o para: {search_term}")
    driver = setup_driver()
    try:
        products = scrape_products(driver, search_term)
        if products:
            save_to_excel(products)
            save_to_json(products)
            print(f"Total de {len(products)} produtos extra√≠dos.")
        else:
            print("Nenhum produto encontrado.")
    finally:
        driver.quit()

